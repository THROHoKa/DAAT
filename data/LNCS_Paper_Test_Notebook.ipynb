#!/usr/bin/env python
# coding: utf-8

# In[50]:


# Imports
import random
import pandas as pd
import numpy as np

# DAAT Imports
import daat
from daat import Instruction as daat_ins
from daat import daat_test

# TQDM
from tqdm.auto import tqdm, trange

# sklearn
import sklearn.svm as skl_svm
import sklearn.preprocessing as skl_prep

from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline

import tensorflow as tf


# # Test 1: NASA KC1 Dataset

# In[2]:


# globale Einstellungen
path_ds_nasa = "./data/kc1.csv"
ds_kc1 = pd.read_csv(path_ds_nasa)


# In[3]:


# Analyser
an_nasa = daat.Analyser(ds_kc1)
an_nasa.drop_line([0,1])
an_nasa.transform_feature('defects')
an_nasa.show_dataset_info()


# In[4]:


ds_nasa_an = an_nasa.dataset
_, _ ,ds_nasa_gen = daat_test.split_data_from_all(ds_nasa_an, 'defects', 0.7, 0.3)

# Datensatz Augmentation Einstellung
aug_kc1 = daat.Generator(ds_nasa_gen, 'defects')

# Unabh채ngige Werte 체ber Distribution
aug_kc1.add_instruction(daat_ins('total_Op', daat.Gen_Distribution(rng_min=0)))
aug_kc1.add_instruction(daat_ins('total_Opnd', daat.Gen_Distribution(rng_min=0)))

# uniq sollten jeweils kleiner sein als total -> total beachten
aug_kc1.add_instruction(daat_ins('uniq_Op', daat.Gen_NextMean(10), ['total_Op']))
aug_kc1.add_instruction(daat_ins('uniq_Opnd', daat.Gen_NextMean(10), ['total_Opnd']))

aug_kc1.add_instruction(daat_ins('lOCode', daat.Gen_NextMean(10, rng_min=0)))
aug_kc1.add_instruction(daat_ins('lOComment', daat.Gen_Cluster(10, rng_min=0)))
aug_kc1.add_instruction(daat_ins('lOBlank', daat.Gen_Cluster(10, rng_min=0)))
aug_kc1.add_instruction(daat_ins('locCodeAndComment', daat.Gen_None()))

# Alle berechneten Werte mit Spline
aug_kc1.add_instruction(daat_ins('n', daat.Gen_Spline(), ['total_Op', 'total_Opnd']))
aug_kc1.add_instruction(daat_ins('v', daat.Gen_Spline(), 'n'))
aug_kc1.add_instruction(daat_ins('l', daat.Gen_Spline(), ['uniq_Opnd', 'n'])) # berechnet mit v' (aus uniq_Opnd)
aug_kc1.add_instruction(daat_ins('d', daat.Gen_NextMean(5), 'l'))
aug_kc1.add_instruction(daat_ins('i', daat.Gen_NextMean(5), ['uniq_Opnd', 'd'])) # berechnet mit 1/l' (l' = 1/d)
aug_kc1.add_instruction(daat_ins('e', daat.Gen_Spline(), ['v', 'l'])) # berechnet mit l' und v'
aug_kc1.add_instruction(daat_ins('t', daat.Gen_Spline(), 'e'))
aug_kc1.add_instruction(daat_ins('b', daat.Gen_Spline(), ['n', 'v']))

aug_kc1.add_instruction(daat_ins('loc', daat.Gen_Recombine(5, rng_min=0), 'lOCode'))
aug_kc1.add_instruction(daat_ins('branchCount', daat.Gen_Recombine(15,rng_min=0), ['lOCode', 'lOComment', 'lOBlank', 'locCodeAndComment']))
aug_kc1.add_instruction(daat_ins('v(g)', daat.Gen_Spline(), 'branchCount'))
aug_kc1.add_instruction(daat_ins('ev(g)', daat.Gen_Spline(), 'branchCount'))
aug_kc1.add_instruction(daat_ins('iv(g)', daat.Gen_Spline(), 'branchCount'))

# aug_kc1.status()
aug_kc1.verify_setup()#balance=True)


# In[5]:


aug_kc1.generate_syn_data(n_samples=len(ds_nasa_an), equal=True)
ds_aug_kc1 = aug_kc1.get_syn_data()

an_syn = daat.Analyser(ds_aug_kc1)
an_syn.show_dataset_info()


# In[6]:


f_name = 'branchCount'

an_nasa.plot_features(inline = 8, color=True)
an_syn.plot_features(inline = 8, color=True)


# In[7]:


tester = daat.Verification(ds_nasa_an, ds_aug_kc1)
tester.eval_class_data_set('defects', 0.7, weights='balanced')


# In[ ]:





# In[8]:


# Datensatz Einstellungen
PATH_DATASET_KC1 = "./data/kc1.csv"

KC1_COLUMNS = ['loc', 'v(g)', 'ev(g)', 'iv(g)', 'n', 'v', 'l', 'd', 'i', 'e', 'b', 't',
               'lOCode', 'lOComment', 'lOBlank', 'locCodeAndComment', 'uniq_Op',
               'uniq_Opnd', 'total_Op', 'total_Opnd', 'branchCount', 'defects']

KC1_selected  = ['lOCode', 'lOComment', 'lOBlank', 'locCodeAndComment', 'uniq_Op',
                 'uniq_Opnd', 'total_Op', 'total_Opnd', 'branchCount', 'defects']

KC1_TARGET = KC1_COLUMNS[21]

# Datensatz laden
ds_kc1_raw = pd.read_csv(PATH_DATASET_KC1)
ds_kc1_raw.columns = KC1_COLUMNS

# Analyser
an_kc1 = daat.Analyser(ds_kc1_raw, KC1_TARGET)
an_kc1.drop_line([0,1])
an_kc1.transform_feature(KC1_TARGET)


# In[32]:





# In[33]:


# aug_kc1.generate_syn_data(len(ds_kc1_train), balance=True)
# aug_kc1.get_syn_data(combine=True)
# an_kc1_2 = daat.Analyser(aug_kc1.get_syn_data(combine=True), KC1_TARGET)


# In[53]:


import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn import svm
from sklearn.model_selection import cross_val_score
import sklearn.metrics as skl_m

from sklearn.utils.class_weight import compute_class_weight

train_epochs = 100
n_labels = 2
callback_early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)

def test_ffann_10_100(ratio):
    runs = 100
    ds_kc1_an = an_kc1.dataset
    ds_kc1_train, ds_kc1_test, ds_kc1_gen = daat_test.split_data_from_train(ds_kc1_an, KC1_TARGET,0.7,ratio)

    # Datensatz Augmentation Einstellung
    aug_kc1 = daat.Generator(ds_kc1_gen, 'defects')

    # Unabh채ngige Werte 체ber Distribution
    aug_kc1.add_instruction(daat_ins('total_Op', daat.Gen_Distribution(rng_min=0)))
    aug_kc1.add_instruction(daat_ins('total_Opnd', daat.Gen_Distribution(rng_min=0)))

    # uniq sollten jeweils kleiner sein als total -> total beachten
    aug_kc1.add_instruction(daat_ins('uniq_Op', daat.Gen_NextMean(10), ['total_Op']))
    aug_kc1.add_instruction(daat_ins('uniq_Opnd', daat.Gen_NextMean(10), ['total_Opnd']))

    aug_kc1.add_instruction(daat_ins('lOCode', daat.Gen_NextMean(10, rng_min=0)))
    aug_kc1.add_instruction(daat_ins('lOComment', daat.Gen_Cluster(10, rng_min=0)))
    aug_kc1.add_instruction(daat_ins('lOBlank', daat.Gen_Cluster(10, rng_min=0)))
    aug_kc1.add_instruction(daat_ins('locCodeAndComment', daat.Gen_None()))

    # Alle berechneten Werte mit Spline
    aug_kc1.add_instruction(daat_ins('n', daat.Gen_Spline(), ['total_Op', 'total_Opnd']))
    aug_kc1.add_instruction(daat_ins('v', daat.Gen_Spline(), 'n'))
    aug_kc1.add_instruction(daat_ins('l', daat.Gen_Spline(), ['uniq_Opnd', 'n'])) # berechnet mit v' (aus uniq_Opnd)
    aug_kc1.add_instruction(daat_ins('d', daat.Gen_NextMean(5), 'l'))
    aug_kc1.add_instruction(daat_ins('i', daat.Gen_NextMean(5), ['uniq_Opnd', 'd'])) # berechnet mit 1/l' (l' = 1/d)
    aug_kc1.add_instruction(daat_ins('e', daat.Gen_Spline(), ['v', 'l'])) # berechnet mit l' und v'
    aug_kc1.add_instruction(daat_ins('t', daat.Gen_Spline(), 'e'))
    aug_kc1.add_instruction(daat_ins('b', daat.Gen_Spline(), ['n', 'v']))

    aug_kc1.add_instruction(daat_ins('loc', daat.Gen_Recombine(5, rng_min=0), 'lOCode'))
    aug_kc1.add_instruction(daat_ins('branchCount', daat.Gen_Recombine(15,rng_min=0), ['lOCode', 'lOComment', 'lOBlank', 'locCodeAndComment']))
    aug_kc1.add_instruction(daat_ins('v(g)', daat.Gen_Spline(), 'branchCount'))
    aug_kc1.add_instruction(daat_ins('ev(g)', daat.Gen_Spline(), 'branchCount'))
    aug_kc1.add_instruction(daat_ins('iv(g)', daat.Gen_Spline(), 'branchCount'))

    # Angegbene Instruktionen testen
    # aug_kc1.verify_setup(balance=True)
    
    acc = np.zeros((3, runs))
    pre = np.zeros((3, runs))
    re  = np.zeros((3, runs))
    f1  = np.zeros((3, runs))

    for i in trange(runs):
        s_aug = (len(ds_kc1_train) - len(ds_kc1_gen))
        aug_kc1.generate_syn_data(s_aug, balance=True)
        ds_aug_kc1 = aug_kc1.get_syn_data(combine=True)

        x_train_org = ds_kc1_train.drop(KC1_TARGET, axis=1).to_numpy()
        y_train_org = ds_kc1_train[KC1_TARGET].to_numpy()
        org_weights = dict(enumerate(compute_class_weight('balanced', classes=np.unique(y_train_org), y=y_train_org)))
        model_org = daat_test.create_custom_class_model(x_train_org.shape[1], n_labels, n=3, nodes=8)
        model_org.fit(x_train_org, y_train_org, epochs=train_epochs, verbose=0, class_weight=org_weights, callbacks=[callback_early])

        x_train_gen = ds_kc1_gen.drop(KC1_TARGET, axis=1).to_numpy()
        y_train_gen = ds_kc1_gen[KC1_TARGET].to_numpy()
        gen_weights = dict(enumerate(compute_class_weight('balanced', classes=np.unique(y_train_gen), y=y_train_gen)))
        model_gen = daat_test.create_custom_class_model(x_train_gen.shape[1], n_labels, n=3, nodes=8)
        model_gen.fit(x_train_gen, y_train_gen, epochs=train_epochs, verbose=0, class_weight=gen_weights, callbacks=[callback_early])

        x_train_aug = ds_aug_kc1.drop(KC1_TARGET, axis=1).to_numpy()
        y_train_aug = ds_aug_kc1[KC1_TARGET].to_numpy()
        aug_weights = dict(enumerate(compute_class_weight('balanced', classes=np.unique(y_train_aug), y=y_train_aug)))
        model_aug = daat_test.create_custom_class_model(x_train_aug.shape[1], n_labels, n=3, nodes=8)
        model_aug.fit(x_train_aug, y_train_aug, epochs=train_epochs, verbose=0, class_weight=aug_weights, callbacks=[callback_early])

        x_test = ds_kc1_test.drop(KC1_TARGET, axis=1).to_numpy()
        y_test = ds_kc1_test[KC1_TARGET].to_numpy()

        org_loss, org_acc = model_org.evaluate(x_test, y_test, verbose=0)
        org_pred = np.argmax(model_org.predict(x_test, verbose=0), axis = 1)
        acc[0,i] = skl_m.accuracy_score(y_test, org_pred)
        pre[0,i] = skl_m.precision_score(y_test, org_pred, zero_division=0)
        re [0,i] = skl_m.recall_score(y_test, org_pred, zero_division=0)
        f1 [0,i] = skl_m.f1_score(y_test, org_pred, zero_division=0)

        aug_loss, aug_acc = model_aug.evaluate(x_test, y_test, verbose=0)
        aug_pred = np.argmax(model_aug.predict(x_test, verbose=0), axis = 1)
        acc[1,i] = skl_m.accuracy_score(y_test, aug_pred)
        pre[1,i] = skl_m.precision_score(y_test, aug_pred, zero_division=0)
        re [1,i] = skl_m.recall_score(y_test, aug_pred, zero_division=0)
        f1 [1,i] = skl_m.f1_score(y_test, aug_pred, zero_division=0)

        gen_loss, gen_acc = model_gen.evaluate(x_test, y_test, verbose=0)
        gen_pred = np.argmax(model_gen.predict(x_test, verbose=0), axis = 1)
        acc[2,i] = skl_m.accuracy_score(y_test, gen_pred)
        pre[2,i] = skl_m.precision_score(y_test, gen_pred, zero_division=0)
        re [2,i] = skl_m.recall_score(y_test, gen_pred, zero_division=0)
        f1 [2,i] = skl_m.f1_score(y_test, gen_pred, zero_division=0)
        
        if i == 9 or i == 99:
            run = i+1
            print(acc[0,:run])
            print('='* 70)
            print('Evaluation', str(run), 'runs')
            print('='* 70)
            print('org_acc: %.3f' % (np.average(acc[0,:run])), '|', 'gen_acc: %.3f' % (np.average(acc[2,:run])), '|', 'syn_acc: %.3f' % (np.average(acc[1,:run])))
            print('org_pre: %.3f' % (np.average(pre[0,:run])), '|', 'gen_pre: %.3f' % (np.average(pre[2,:run])), '|', 'syn_pre: %.3f' % (np.average(pre[1,:run])))
            print('org_re:  %.3f' % (np.average(re[0,:run])) , '|', 'gen_re:  %.3f' % (np.average(re[2,:run])),  '|', 'syn_re:  %.3f' % (np.average(re[1,:run])))
            print('org_f1:  %.3f' % (np.average(f1[0,:run])) , '|', 'gen_f1:  %.3f' % (np.average(f1[2,:run])),  '|', 'syn_f1:  %.3f' % (np.average(f1[1,:run])))
            print('='* 70)


# In[54]:


test_ffann_10_100(0.1)
test_ffann_10_100(0.2)
test_ffann_10_100(0.4)


# # Test 2: Pima Indians Diabetes Database

# In[8]:


# globale Einstellungen
path_ds_diabetes = "./data/diabetes_pima.csv"
ds_diabetes = pd.read_csv(path_ds_diabetes, header=None)

ds_diabetes.columns = ['n_preg', 'plasma_con', 'blood_mm', 'skin_mm', 'insulin', 'bmi', 'dpf', 'age', 'class']
ds_diabetes = ds_diabetes.drop(ds_diabetes[ds_diabetes.age == 0.0].index)
ds_diabetes = ds_diabetes.drop(ds_diabetes[ds_diabetes.blood_mm == 0.0].index)
ds_diabetes = ds_diabetes.drop(ds_diabetes[ds_diabetes.skin_mm == 0.0].index)
ds_diabetes = ds_diabetes.drop(ds_diabetes[ds_diabetes.insulin == 0.0].index)
ds_diabetes = ds_diabetes.drop(ds_diabetes[ds_diabetes.bmi == 0.0].index)

an_diabetes = daat.Analyser(ds_diabetes, 'class')
an_diabetes.show_dataset_info()
# an_diabetes.plot_features(y_feature='blood_mm', color=True)
# an_diabetes.plot_correlation()


# In[9]:


def advanced_performance_measures(predicted, truth):
    tp = tn = fp = fn = 0

    for i in range(0, len(predicted)):
        if (predicted[i] == 1 and truth[i] == 1): tp += 1
        elif (predicted[i] == 0 and truth[i] == 0): tn += 1
        elif (predicted[i] == 1 and truth[i] == 0): fp += 1
        elif (predicted[i] == 0 and truth[i] == 1): fn += 1

    total = tp + tn + fp + fn
    acc = (tp + tn) / (total)

    pos = tp + fp
    tru = tp + fn

    pre = 0 if pos == 0 else (tp) / (tp + fp)
    rec = 0 if tru == 0 else (tp) / (tp + fn)

    f_s = pre + rec

    f1 = 0 if f_s == 0 else 2 * ((pre * rec) / (f_s))

    return [acc, pre, rec, f1]


# specific test for diabetes dataset.
def run_test_class_ratio(data, target, ratio, cycles, balance = False):
    randomlist = random.sample(range(0, 400), cycles)
    n_samples = len(data)
    metrics = np.zeros((cycles, 9, 4))
    
    for c in trange(cycles, desc='running cycle', leave=False):
    
        # Generator Seed Daten nach ratio bestimmen
        data_x = data.drop(target, axis = 1).to_numpy()
        data_y = data[target].to_numpy()

        x_gen, _, y_gen, _ = train_test_split(data_x, data_y, train_size=ratio, 
                                              random_state=randomlist[c], stratify=data_y)
        y_gen = y_gen.reshape(len(y_gen), 1)
        ds_gen_ratio = pd.DataFrame(np.append(x_gen, y_gen, axis = 1), columns=data.columns)

        # Generator Objekte erstellen
        gen_ratio = daat.Generator(ds_gen_ratio, "class")
        gen_100 = daat.Generator(data, "class")

        # Instruktionen hinzuf체gen
        gen_ratio.add_instruction(daat_ins("age", daat.Gen_Distribution(rng_min=21)))
        gen_ratio.add_instruction(daat_ins("n_preg", daat.Gen_Distribution(rng_min=0)))
        gen_ratio.add_instruction(daat_ins("blood_mm", daat.Gen_Recombine(15, rng_min=0), ['age']))
        gen_ratio.add_instruction(daat_ins("skin_mm", daat.Gen_Cluster(10, rng_min=0), ['n_preg', 'age']))
        gen_ratio.add_instruction(daat_ins("insulin", daat.Gen_NextMean(5, rng_min=0), ['skin_mm', 'blood_mm']))
        gen_ratio.add_instruction(daat_ins("plasma_con", daat.Gen_NextMean(10, rng_min=0), ['age','insulin']))
        gen_ratio.add_instruction(daat_ins("bmi", daat.Gen_Recombine(5, rng_min=0), ['insulin', 'skin_mm', 'age']))
        gen_ratio.add_instruction(daat_ins("dpf", daat.Gen_NextMean(5, rng_min=0), ['age', 'n_preg']))

        gen_100.add_instruction(daat_ins("age", daat.Gen_Distribution(rng_min=21)))
        gen_100.add_instruction(daat_ins("n_preg", daat.Gen_Distribution(rng_min=0)))
        gen_100.add_instruction(daat_ins("blood_mm", daat.Gen_Recombine(15, rng_min=0), ['age']))
        gen_100.add_instruction(daat_ins("skin_mm", daat.Gen_Cluster(10, rng_min=0), ['n_preg', 'age']))
        gen_100.add_instruction(daat_ins("insulin", daat.Gen_NextMean(5, rng_min=0), ['skin_mm', 'blood_mm']))
        gen_100.add_instruction(daat_ins("plasma_con", daat.Gen_NextMean(10, rng_min=0), ['age','insulin']))
        gen_100.add_instruction(daat_ins("bmi", daat.Gen_Recombine(5, rng_min=0), ['insulin', 'skin_mm', 'age']))
        gen_100.add_instruction(daat_ins("dpf", daat.Gen_NextMean(5, rng_min=0), ['age', 'n_preg']))

        # Daten generieren
        gen_ratio.generate_syn_data(n_samples, balance=balance)
        ds_data_syn_ratio = gen_ratio.get_syn_data()
        gen_100.generate_syn_data(n_samples, balance=balance)
        ds_data_syn_100 = gen_100.get_syn_data()
        

        # original Daten
        org_x = data.drop(target, axis = 1).to_numpy()
        org_y = data[target].to_numpy()
        org_x_train, org_x_test, org_y_train, org_y_test = train_test_split(
            org_x, org_y, train_size = 0.7, random_state = 42, stratify = org_y)
        
        # syn_ratio
        syn_ratio_x = ds_data_syn_ratio.drop(target, axis = 1).to_numpy()
        syn_ratio_y = ds_data_syn_ratio[target].to_numpy()
        syn_ratio_x_train, syn_ratio_x_test, syn_ratio_y_train, syn_ratio_y_test = train_test_split(
            syn_ratio_x, syn_ratio_y, train_size = 0.7, random_state = 42, stratify = syn_ratio_y)
        
        #syn_100
        syn_100_x = ds_data_syn_100.drop(target, axis = 1).to_numpy()
        syn_100_y = ds_data_syn_100[target].to_numpy()
        syn_100_x_train, syn_100_x_test, syn_100_y_train, syn_100_y_test = train_test_split(
            syn_100_x, syn_100_y, train_size = 0.7, random_state = 42, stratify = syn_100_y)
        
        # Erstellen der drei Test-SVMs
        svm_org = make_pipeline(skl_prep.StandardScaler(), skl_svm.SVC(cache_size = 200, max_iter = 10000))
        svm_org.fit(org_x_train, org_y_train)
        
        svm_syn_ratio = make_pipeline(skl_prep.StandardScaler(), skl_svm.SVC(cache_size = 200, max_iter = 10000))
        svm_syn_ratio.fit(syn_ratio_x_train, syn_ratio_y_train)
        
        svm_syn_100 = make_pipeline(skl_prep.StandardScaler(), skl_svm.SVC(cache_size = 200, max_iter = 10000))
        svm_syn_100.fit(syn_100_x_train, syn_100_y_train)
        
        pred_org_o     = svm_org.predict(org_x_test)
        pred_org_srat  = svm_org.predict(syn_ratio_x_test)
        pred_org_s100  = svm_org.predict(syn_100_x_test)
        
        pred_srat_o    = svm_syn_ratio.predict(org_x_test)
        pred_srat_srat = svm_syn_ratio.predict(syn_ratio_x_test)
        pred_srat_s100 = svm_syn_ratio.predict(syn_100_x_test)
        
        pred_s100_o    = svm_syn_100.predict(org_x_test)
        pred_s100_srat = svm_syn_100.predict(syn_ratio_x_test)
        pred_s100_s100 = svm_syn_100.predict(syn_100_x_test)
         
        metrics[c,0] = advanced_performance_measures(pred_org_o, org_y_test)
        metrics[c,1] = advanced_performance_measures(pred_org_srat, syn_ratio_y_test)
        metrics[c,2] = advanced_performance_measures(pred_org_s100, syn_100_y_test)
        
        metrics[c,3] = advanced_performance_measures(pred_srat_o, org_y_test)
        metrics[c,4] = advanced_performance_measures(pred_srat_srat, syn_ratio_y_test)
        metrics[c,5] = advanced_performance_measures(pred_srat_s100, syn_100_y_test)
        
        metrics[c,6] = advanced_performance_measures(pred_s100_o, org_y_test)
        metrics[c,7] = advanced_performance_measures(pred_s100_srat, syn_ratio_y_test)
        metrics[c,8] = advanced_performance_measures(pred_s100_s100, syn_100_y_test)
        
    # TABELLE
    tab_head_1 = '| {:10} || {:30} || {:30} || {:30} |'
    tab_head_2 = '| {:10} || {:8} | {:8} | {:8} || {:8} | {:8} | {:8} || {:8} | {:8} | {:8} |'
    tab_col = ['Metric', 'Accuracy', 'Precision', 'Recall', 'F1']
    tab_head_col = ['Modell', '            OG', '           Syn 10', '          Syn 100']
    header_syn = 'Syn ' + str(100*ratio) 
    tab_head_col_2 = ['Data Set', 'OG 30', header_syn, 'Syn 100', 'OG 30', header_syn, 'Syn 100', 'OG 30', header_syn, 'Syn 100']
    tab_line_1 = '| {:10} || {:8.3f} | {:8.3f} | {:8.3f} || {:8.3f} | {:8.3f} | {:8.3f} || {:8.3f} | {:8.3f} | {:8.3f} |'
    
    line_len = 9*10 + 14 + 12
    print('=' * line_len)
    print('Data Set Evaluation for', cycles, 'Cycles, ', ratio, 'Seed Data')
    print('-' * line_len)
    print(tab_head_1.format(*tab_head_col))
    print(tab_head_2.format(*tab_head_col_2))
    print('-' * line_len)

    for i in range(0, len(metrics[0,0])):
        print(tab_line_1.format(tab_col[i+1], 
                                (np.sum(metrics[:,0,i])/cycles), (np.sum(metrics[:,1,i])/cycles), (np.sum(metrics[:,2,i])/cycles), 
                                (np.sum(metrics[:,3,i])/cycles), (np.sum(metrics[:,4,i])/cycles), (np.sum(metrics[:,5,i])/cycles), 
                                (np.sum(metrics[:,6,i])/cycles), (np.sum(metrics[:,7,i])/cycles), (np.sum(metrics[:,8,i])/cycles)
             ))

    print('=' * line_len)


# In[10]:


run_test_class_ratio(ds_diabetes, 'class', 0.1, 10, balance=True)
run_test_class_ratio(ds_diabetes, 'class', 0.2, 10, balance=True)
run_test_class_ratio(ds_diabetes, 'class', 0.4, 10, balance=True)
run_test_class_ratio(ds_diabetes, 'class', 0.6, 10, balance=True)
run_test_class_ratio(ds_diabetes, 'class', 0.8, 10, balance=True)

